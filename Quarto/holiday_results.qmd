---
title: "Modelling between study hetrogeneity when developing prediction models for IPD meta-analysis"
format: html
editor: visual
execute: 
  echo: false
  warning: false
---

```{r}
devtools::load_all(path = here::here())

library(tidyverse)
library(DBI)
library(RSQLite)
library(here)

dplyr.summarise.inform = FALSE
# Connecting to database
db <- dbConnect(RSQLite::SQLite(), here("Results/Database/sim_results.db"))



```

New addition was calcualtion of bias for beta covariates to investiage why we are seeing no impact on performance from random effects models when we introduce level 2 endogenity.

Simulation scenarios:

## Simulation scenarios run

Scenarios run factorially. Note, when the ICC is zero, the intercept-predictor correlation is zero as there is no between study variability in intercepts.

```{r, echo = FALSE}


scenarios <- dbGetQuery(db,
            "SELECT  DISTINCT n_studies, ICC, R2, study_sample_size_train , int_pred_corr
            FROM sim_results_v2
            WHERE sim_name = 'BatchAug2023'  AND metric = 'calib_itl'"
)
n_studies <- paste(scenarios$n_studies |> unique(), collapse = ", ")
study_sample_size <- paste(scenarios$study_sample_size_train |> unique(), collapse = ", ")
ICC <- paste(scenarios$ICC |> unique(), collapse = ", ")
R2 <- paste(scenarios$R2 |> unique(), collapse = ", ")
int_pred_corr <- paste(scenarios$int_pred_corr |> unique(), collapse = ", ")
tibble(Parameter = c("N studies", "Study sample size", "ICC", "R2", "Intercept-predictor corr"), Values = c(n_studies, study_sample_size, ICC, R2, int_pred_corr))

metrics <- dbGetQuery(db,
            "SELECT  DISTINCT metric
            FROM sim_results_v2
            WHERE sim_name = 'BatchAug2023'"
)

tibble(metrics)

```

### Data generating parameters

#### When there is no correleation between intercepts and predictors

```{r}
bind_rows(
  get_sigmas(ICC = 0.05, R2 = 0.4, int_pred_corr = 0, n_predictors = 12),
  get_sigmas(ICC = 0.3, R2 = 0.4, int_pred_corr = 0, n_predictors = 12),
  get_sigmas(ICC = 0.05, R2 = 0.7, int_pred_corr = 0, n_predictors = 12),
  get_sigmas(ICC = 0.3, R2 = 0.7, int_pred_corr = 0, n_predictors = 12)
) |> 
  mutate(
    ICC = rep(c(0.05, 0.3),2),
    R2 = c(0.4, 0.4, 0.7, 0.7),
    predictor_explained_var = 12*beta_x^2,
    outcome_var = predictor_explained_var + u^2 + e^2 ,
    r2_check = (u^2 + predictor_explained_var)/(outcome_var) - R2 
  ) |> 
  select(R2, ICC, sigma_u = u, sigma_e = e, everything())
```

What is going on? For a given ICC sigma_u and sigma_e are fixed to give a variance for the random part of the model of 1 and the required ICC. The variance of individual predicors is 1, so beta is chosed to the required predictors explain the required ammount of the total variance. The variance of the outcome explained by predictors will be \$ 12\*beta_x\^2\$.

#### When there is a correleation between intercepts and predictors

The models data/models have 12 predictors all with the same beta value. Beta, and values of the variances of different components are chosen to give the required r-squared, ICC and correlations These are the parameters used in the models with correlations between intercept and predictors. `beta_int` is a parameter in the data generating model for predictors. When there is less variability in study intercepts (ICC = 0.05) `beta_int` needs to be greater to give the same correlation between predictors and intercepts. Outcomes do not have constant variance.

```{r}
bind_rows(
  get_sigmas(ICC = 0.05, R2 = 0.4, int_pred_corr = 0.5, n_predictors = 12),
  get_sigmas(ICC = 0.3, R2 = 0.4, int_pred_corr = 0.5, n_predictors = 12),
  get_sigmas(ICC = 0.05, R2 = 0.7, int_pred_corr = 0.5, n_predictors = 12),
  get_sigmas(ICC = 0.3, R2 = 0.7, int_pred_corr = 0.5, n_predictors = 12)
) |> 
  mutate(ICC = rep(c(0.05, 0.3),2),
         R2 = c(0.4, 0.4, 0.7, 0.7)) |> 
  select(R2, ICC, sigma_u = u, sigma_e = e, everything())

```

## Random intercept models underestimate variance of intercepts whent there are a small number of studies

```{r, echo = FALSE}

data <- dbGetQuery(db,
           "SELECT n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model, beta_mean_error, beta_x, sigma_e, sigma_u, est, test_ss, n_studies_test, metric, tau2 rep_number, error_var_u
            FROM sim_results_v2
            WHERE sim_name = 'BatchAug2023' AND metric = 'var_u' AND est IS NOT NULL AND int_pred_corr =0"
) 

```

```{r, fig.height = 6}
data |> 
  dplyr::filter(R2 == 0.4) |> 
  mutate(error_var_u = est - sigma_u^2) |> 
  group_by(n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model)  |> 
  summarise(
    value = mean(error_var_u), 
    ll = robust_t_test(error_var_u)[1], 
    ul = robust_t_test(error_var_u)[2],  
    n = sum(!is.na(error_var_u))
  ) |> 
  ProfacSims:::plot_error_var_u()
```

### Impact on predictive performance: Calibration in the large

```{r, echo = FALSE}
# Processing database results to give summaries and ns
data <- dbGetQuery(db,
           "SELECT n_studies, ICC, R2, study_sample_size_train ,test_ss, int_pred_corr, model, est, tau2, sigma_e, sigma_u, intercept_est_sample_size, metric, n_studies_test
            FROM sim_results_v2
            WHERE sim_name = 'BatchAug2023' AND metric = 'calib_itl' AND int_pred_corr = 0.0" 
)



```

Calibration in the large is good accross all scenarios

```{r, fig.height = 6}
data |> 
  ProfacSims:::plot.sim_results(what = "Performance")

```

There is hetrogeneity for some scenarios:

```{r, fig.height = 12}
data |> 
  ProfacSims:::plot.sim_results(what = "Tau")

```

We would expect poor estiamtion of study intercepts to lead to heterogeneous calibration in the large. There are however different approaches to estimate random intercepts in new samples.

-   Random intercept models can underestimate variance of intercepts when there are a small number of studies
-   This will lead to over-penalisation of predicted intercepts, which in turn will lead to heterogeneity in calibration-in-the large
-   When the ICC is 0, random intercept models penalise estimates of intercepts towards zero, where as fixed- (or no estimate of intercept models) apply no penalisation
-   When ICC \> 0 the penalization of random intercepts leads to better perfmroance.
-   There is some evidence of over penalization - Calib ITL is worse with a small number of studies
-   As the ICC increases the difference between random intercept models and fixed effects models shrinks. This is due to random intercept models applying a lower ammount of penalisation
-   The impact of REML is minimal.
-   Patterns remain if 100 people are avialbe for estimating intercepts but differences are negligible.
-   Tau remains low - what is material? The outcome variance is between 1 and 3 (higher variances for the outcome for higher r-squared = 0.7). When Tau is 0.1 this as is 10% of outcome variance. Tau of 0.1 feel less material as is 1% of outcome variance
-   Conclusions: When there is limited data to estimate new intercepts, random intercept models outperform fixed effect models. When ICCs are moderate this occurs even when there are only 4 studies and random intercept variances are underestimated. When more data is available to estimate interepts, performance is similar for all models, except for 4 studies when radnom intercept models do worse.

### Impact on predictive performance: Calibration slope

Calibration slope are very similar for models adjusting for study. Calibration slope is less than one when sample sizes are smaller due to overfitting. When ICCs are larger, calibration slope is worse. This is due to there being a lower level of explained variation within study, so a hihger sample size being required to ensure good calibration. Choice of method for esitmating intercepts has no bearing on calibraiton slope.

```{r, echo = FALSE}
# Processing database results to give summaries and ns
data <- dbGetQuery(db,
           "SELECT n_studies, ICC, R2, study_sample_size_train ,test_ss, int_pred_corr, model, est, tau2, sigma_e, sigma_u, intercept_est_sample_size, metric, n_studies_test
            FROM sim_results_v2
            WHERE sim_name = 'BatchAug2023' AND metric = 'calib_slope' AND int_pred_corr = 0.0" 
)



```

```{r, fig.height = 12}
data |> 
  ProfacSims:::plot.sim_results(what = "Performance")

```

```{r, fig.height = 12}
data |> 
  ProfacSims:::plot.sim_results(what = "Tau")
```

Between study heterogeneity in calibration slope is minimal.

### Impact on predictive performance: R-sqared

We see a mixture of the impact on calibration in the large and calibration slope. Hetrogeneity in intercept estimation leads to differences in r-squared.

```{r, echo = FALSE}
# Processing database results to give summaries and ns
data <- dbGetQuery(db,
           "SELECT n_studies, ICC, R2, study_sample_size_train ,test_ss, int_pred_corr, model, est, tau2, sigma_e, sigma_u, intercept_est_sample_size, metric, n_studies_test
            FROM sim_results_v2
            WHERE sim_name = 'BatchAug2023' AND metric = 'r-squared' AND int_pred_corr = 0.0" 
)



```

```{r, fig.height = 12}
data |> 
  ProfacSims:::plot.sim_results(what = "Performance")

```

```{r, fig.height = 12}
data |> 
  ProfacSims:::plot.sim_results(what = "Tau")
```

```{r, echo = FALSE}
# Processing database results to give summaries and ns
data <- dbGetQuery(db,
           "SELECT n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model, beta_mean_error, beta_x, sigma_e, sigma_u
            FROM sim_results_v2
            WHERE sim_name = 'BatchAug2023' AND metric = 'var_u'"
)

# plot n
summaries <- data |> 
  mutate(perc_beta_error = beta_mean_error/beta_x*100) |> 
  group_by(n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model) |> 
  summarise(value = mean(perc_beta_error), 
            ll = robust_t_test(perc_beta_error)[1], 
            ul = robust_t_test(perc_beta_error)[2],  
            n = sum(!is.na(perc_beta_error)),
            beta_x = mean(beta_x)) 


summaries_n <- summaries |> 
  select(-value) |> 
  rename(value = n) 



```

## Number of models that converge

Should be 200 (the number of simulation reps)

```{r, fig.height = 6}
summaries_n |> 
  ProfacSims:::plot_results_by_model() + ylab("N") + scale_y_continuous(breaks = 200)
```

## Bias in beta

## Bias in betas

We expect the bias in betas to be zero for all models when there is no correlation between intercepts and predictors. When there is a correlation we expect bias when:

1.  Study is not adjusted for, as the correlation between study intercepts and predictors will lead to bias in estiamtes of beta (confounding to use causal language).
2.  Maximum liklihood is used for estiamting random intercepts models and the number of studies is low (due to underestimation of between study variance, and consequently under-adjsutment for study)
3.  All random intercept models due to level 2 endogeneity

The following plots examine the difference between the mean estimated betas and the true value for betas. They show percentage bias in beta on the y-axis. Each model has 12 beta parameters, that should be equal. The plots takes the mean across all beta parameters.

```{r, fig.height = 9}
summaries |> filter(int_pred_corr==0) |> plot_results_by_model(CI = TRUE) + ylab("Percentage bias in beta")

```

### No correlation between predictors and study intercepts

Negligible bias for all scenarios \### Correlation between predictors and study intercepts (level 2 endogeneity)

-   Logistic regression with fixed intercepts gives unbiased betas accross all scenarios.
-   No adjusting for study leads to a large bias When using random intercept models bias reduces with stidy size, to become almost zero for study size 1000.
-   Bias, as a percentage of beta. is smaller with higher R-squared as in these scenarios beta is larger. the absolute value of the bias is similar.
-   The bias that comes from not adjusting for study increases with the number of studies - I don't understand why.

```{r, fig.height = 6}
summaries |> filter(int_pred_corr==0.5, ICC !=0) |>  plot_results_by_model(CI = TRUE) + ylab("Percentage bias in beta")

```

### Absolute bias - similar bias for different r-squared

```{r}
summaries <- data |> 
  group_by(n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model) |> 
  summarise(value = mean(beta_mean_error), 
            ll = robust_t_test(beta_mean_error)[1], 
            ul = robust_t_test(beta_mean_error)[2],  
            n = sum(!is.na(beta_mean_error)),
            beta_x = mean(beta_x))
            
summaries |> 
  filter(int_pred_corr == 0.5, ICC !=0) |> 
  plot_results_by_model(CI = TRUE) + ylab("Bias in beta")
```

## What about mean squared error in beta estimates?

Prediction error will be related to the mean squared error in beta-coefficients. This will incorporate both the bias in coefficients and the variance. If it is the case that the variance is much larger than the bias, the mean squared error will be dominated by variance , and relatively low ammounts of bias will become negligible. *(MSE = sum of variance and suqare of bias in estimator)*

This is the case - MSE is similar accross all scenarios that adjust for study, although fixed intercept logistic regression does best.

```{r, echo = FALSE}
data <- dbGetQuery(db,
           "SELECT n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model, beta_mean_error, beta_mse, sigma_e, sigma_u
            FROM sim_results_v2
            WHERE sim_name = 'BatchAug2023' AND metric = 'var_u'"
)

# plot n
summaries <- data |> 
  group_by(n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model) |> 
  summarise(value = mean(beta_mse), 
            ll = robust_t_test(beta_mse)[1], 
            ul = robust_t_test(beta_mse)[2],  
            n = sum(!is.na(beta_mse)),
            beta_x = mean(beta_mse))


summaries |> 
  filter(int_pred_corr == 0.5, ICC !=0) |> 
  plot_results_by_model(CI = TRUE) + ylab("MSE Beta")




```

## What about predictive performance...

Results shown for prediction in new samples where 1000 observations are used to estaimte the intercept - this is optimal.

### Check ns again

Extra step in predidcion pipeline - meta-analyzing model results

```{r, fig.height = 6}

data <- dbGetQuery(db,
           "SELECT n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model, metric, est
            FROM sim_results_v2
            WHERE sim_name = 'BatchAug2023' AND intercept_est_sample_size = 1000 AND (metric = 'calib_slope' OR metric = 'r-squared')"
)

data <- data |> filter(ICC !=0, int_pred_corr == 0.5)

summaries_calib_slope <- data |>
  filter(metric == "calib_slope") |> 
  group_by(n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model) |> 
  summarise(value = mean(est), 
            ll = robust_t_test(est)[1], 
            ul = robust_t_test(est)[2],  
            n = sum(!is.na(est)))


summaries_r2 <- data |>
  filter(metric == "r-squared") |> 
  group_by(n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model) |> 
  summarise(value = mean(est), 
            ll = robust_t_test(est)[1], 
            ul = robust_t_test(est)[2],  
            n = sum(!is.na(est)))



summaries_n <- summaries_calib_slope |> 
  select(-value) |> 
  rename(value = n) 


summaries_n |> 
  ProfacSims:::plot_results_by_model() + ylab("N") + scale_y_continuous(breaks = 200)
```

### R-squared

note performance calculated within study. This leads to lower r-squared than the overall r-sqaured as portion of variance taht is due to study intercepts is not included.

```{r, fig.height = 6}

summaries_r2 |> 
  plot_results_by_model(CI = TRUE) + ylab("R-squared")

```

### Calibrtaion slope

-   better calibration from fixed intercept models
-   Bias from level 2 endogeneity has more of an impact on calibraiton than r-squared
-   This is because estimates of betas are biased upwards. This leads to more extreme predictions for people with extreme values of x, leading to calibration slope less than 1 (note to self - calibration has observed as outcome and predicted as predictor).

```{r, fig.height = 6}

summaries_calib_slope |> 
  plot_results_by_model(CI = TRUE) + ylab("Calibration Slope")

```

## Conclusions

1.  When there is level 2 endogeneity we see bias in beta estimates.

This bias does not reduce with an increasing number of studies but it does reduce with higher study size. This is consistent with the literature, which shows (Castelano 2014) (analytically shown for generalised least squared estimates in Maddala 1971). Intuitively the random intercept estimate of beta can be seen as pooling between cluster and within cluster info in beta. Bias arises from correlation between the between cluster variation in x and intercepts. This leads to bias in the between cluster estimates of beta. When cluster size is large, little weight is given to between cluster estimates and the unbiased within cluster estimates dominate.

2.  In these simulations, sometimes bias was greater when ICC = 0.05 compared to ICC = 0.3. This may be an artifact of a greater assosiation needed between predictors and intercepts to give the same correlation when intercepts were less variable.

3.  The impact of bias arising form level 2 endogeneity on mean squared error for betas, and on overall R-squared is small.

There is however an impact on calibration slope.

3.  Differences between the handling of between- and within- study info for fixed effects models and random intercept models are important

On reflection (and reading RHS section 3.8 p158), the data generating model chosen for my simulations favors fixed effects models due to predictors varying within clusters but not between clusters. Fixed effects models only use within cluster info, this is large in my data as this is where the variation in predictors occurs. If there is more between cluster variation in predictors (and within cluster and between cluster effects are identical/similar) then the random intercept model can have lower mean squared error as it can make use of between cluster info. This would occur if clusters were homogeneous but different, for example if we have studies recruited based on intellectual disability with little variation within clusters. I also assume constant within and between cluster effects. If there is a differing between cluster effect of a predictor (how could this occur - could be driven by selection eg. clinical samples have lower IQ, but also worse outcomes because of other unmeasured factors which are lower in clinical samples)

Recommendations as to which model to use should be driven by whether level 2 endogeneity is thought to be a problem (an additional assumption from random intercept models), but also whether there is between cluster information available. This will particularly be the case when there are lots of smaller clusters. In general for IPD meta-analysis there is more variation within studies than between studies and level 2 endogeneity is likely to be an issue as differing selection may have occurred \[find papers\].

The situation may be more complicated if random slopes for predictors are included, as these may remain biased due to level 2 endogeneity.

4.  Other thoughts

To many dimensions to think about. What about bias variance. Think about searching for daylight between FE and RE models. What are the impliacitons for individual level prediction of between-study information - does this fundamentally focus on within cluster effects? What if the variance of predictors varies with intercept.

```{r}
dbDisconnect(db)

```

---
title: "Modelling between study hetrogeneity when developing prediction models for IPD meta-analysis"
format: html
editor: visual
execute: 
  echo: false
  warning: false
---

```{r}
devtools::load_all(path = here::here())

library(tidyverse)
library(DBI)
library(RSQLite)
library(here)

dplyr.summarise.inform = FALSE


```

## Intro

```{r, echo = FALSE}
# Processing database results to give summaries and ns
load(here("Results/sim_results_r2-25_nrep100_23-07-20_01-30"))
data <- tibble(sim_results_new) |> 
  mutate(int_ss = test_ss/n_studies_test) |> 
  filter(int_pred_corr==0)

# Pick an R-squared, pick an ICC to tell the story

data <- data |> filter(R2 == 0.7, ICC == 0.3)
  
# Get data for int_pred_corr = 0

# plot n
summaries <- data |> 
  group_by(n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model, intercept_est_sample_size) |> 
  summarise(value = mean(tau2), 
            ll = robust_t_test(tau2)[1], 
            ul = robust_t_test(tau2)[2],  
            n = sum(!is.na(tau2)))

summaries |> 
  ProfacSims:::plot_results_by_pred() + ylab("Tau-squared") 

```

Simulation scenarios:




### Data generating parameters

The models data/models have 12 predictors all with the same beta value. Beta, and values of the variances of different components are chosen to give the required r-squared, ICC and correlations These are the parameters used in the models with correlations between intercept and predictors. `beta_int` is a parameter in the data generating model for predictors. When there is less variability in study intercepts (ICC = 0.05) `beta_int` needs to be greater to give the same correlation between predictors and intercepts. Outcomes do not have constant variance.

```{r}
bind_rows(
  get_sigmas(ICC = 0.05, R2 = 0.4, int_pred_corr = 0.5, n_predictors = 12),
  get_sigmas(ICC = 0.3, R2 = 0.4, int_pred_corr = 0.5, n_predictors = 12),
  get_sigmas(ICC = 0.05, R2 = 0.7, int_pred_corr = 0.5, n_predictors = 12),
  get_sigmas(ICC = 0.3, R2 = 0.7, int_pred_corr = 0.5, n_predictors = 12)
) |> 
  mutate(ICC = rep(c(0.05, 0.3),2),
         R2 = c(0.4, 0.4, 0.7, 0.7)) |> 
  select(R2, ICC, sigma_u = u, sigma_e = e, everything())

```

# plot n
summaries <- data |> 
  group_by(n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model, test_ss) |> 
  filter(metric == "calib_itl") |> 
  summarise(value = mean(est), 
            ll = robust_t_test(est)[1], 
            ul = robust_t_test(est)[2],  
            n = sum(!is.na(est)))


summaries_n <- summaries |> 
  select(-value) |> 
  rename(value = n) 



```

## Number of models that converge

Should be 200 (the number of simulation reps)

```{r, fig.height = 6}
summaries_n |> 
  ProfacSims:::plot_results_by_model() + ylab("N") + scale_y_continuous(breaks = 200)
```

## Calibration in the large.


```{r, fig.height = 9}
summaries |> filter(int_pred_corr==0) |> plot_results_by_model(CI = TRUE) + ylab("Percentage bias in beta")

```

### Correlation between predictors and study intercepts (level 2 endogeneity)

-   Logistic regression with fixed intercepts gives unbiased betas accross all scenarios.
-   No adjusting for study leads to a large bias When using random intercept models bias reduces with stidy size, to become almost zero for study size 1000.
-   Bias, as a percentage of beta. is smaller with higher R-squared as in these scenarios beta is larger. the absolute value of the bias is similar.
-   The bias that comes from not adjusting for study increases with the number of studies - I don't understand why.

```{r, fig.height = 6}
summaries |> filter(int_pred_corr==0.5, ICC !=0) |>  plot_results_by_model(CI = TRUE) + ylab("Percentage bias in beta")

```

### Absolute bias - similar bias for different r-squared

```{r}
summaries <- data |> 
  group_by(n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model) |> 
  summarise(value = mean(beta_mean_error), 
            ll = robust_t_test(beta_mean_error)[1], 
            ul = robust_t_test(beta_mean_error)[2],  
            n = sum(!is.na(beta_mean_error)),
            beta_x = mean(beta_x))
            
summaries |> 
  filter(int_pred_corr == 0.5, ICC !=0) |> 
  plot_results_by_model(CI = TRUE) + ylab("Bias in beta")
```

## What about mean squared error in beta estimates?

Prediction error will be related to the mean squared error in beta-coefficients. This will incorporate both the bias in coefficients and the variance. If it is the case that the variance is much larger than the bias, the mean squared error will be dominated by variance , and relatively low ammounts of bias will become negligible. *(MSE = sum of variance and suqare of bias in estimator)*

This is the case - MSE is similar accross all scenarios that adjust for study, although fixed intercept logistic regression does best.

```{r, echo = FALSE}
data <- dbGetQuery(db,
           "SELECT n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model, beta_mean_error, beta_mse, sigma_e, sigma_u
            FROM sim_results_v1
            WHERE sim_name = 'BatchAug2023' AND metric = 'var_u'"
)

# plot n
summaries <- data |> 
  group_by(n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model) |> 
  summarise(value = mean(beta_mse), 
            ll = robust_t_test(beta_mse)[1], 
            ul = robust_t_test(beta_mse)[2],  
            n = sum(!is.na(beta_mse)),
            beta_x = mean(beta_mse))


summaries |> 
  filter(int_pred_corr == 0.5, ICC !=0) |> 
  plot_results_by_model(CI = TRUE) + ylab("MSE Beta")




```

## What about predictive performance...

Results shown for prediction in new samples where 1000 observations are used to estaimte the intercept - this is optimal.

### Check ns again

Extra step in predidcion pipeline - meta-analyzing model results

```{r, fig.height = 6}

data <- dbGetQuery(db,
           "SELECT n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model, metric, est
            FROM sim_results_v1
            WHERE sim_name = 'BatchAug2023' AND intercept_est_sample_size = 1000 AND (metric = 'calib_slope' OR metric = 'r-squared')"
)

data <- data |> filter(ICC !=0, int_pred_corr == 0.5)

summaries_calib_slope <- data |>
  filter(metric == "calib_slope") |> 
  group_by(n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model) |> 
  summarise(value = mean(est), 
            ll = robust_t_test(est)[1], 
            ul = robust_t_test(est)[2],  
            n = sum(!is.na(est)))


summaries_r2 <- data |>
  filter(metric == "r-squared") |> 
  group_by(n_studies, ICC, R2, study_sample_size_train , int_pred_corr, model) |> 
  summarise(value = mean(est), 
            ll = robust_t_test(est)[1], 
            ul = robust_t_test(est)[2],  
            n = sum(!is.na(est)))



summaries_n <- summaries_calib_slope |> 
  select(-value) |> 
  rename(value = n) 


summaries_n |> 
  ProfacSims:::plot_results_by_model() + ylab("N") + scale_y_continuous(breaks = 200)
```

### R-squared

note performance calculated within study. This leads to lower r-squared than the overall r-sqaured as portion of variance taht is due to study intercepts is not included.

```{r, fig.height = 6}

summaries_r2 |> 
  plot_results_by_model(CI = TRUE) + ylab("R-squared")

```

### Calibrtaion slope

-   better calibration from fixed intercept models
-   Bias from level 2 endogeneity has more of an impact on calibraiton than r-squared
-   This is because estimates of betas are biased upwards. This leads to more extreme predictions for people with extreme values of x, leading to calibration slope less than 1 (note to self - calibration has observed as outcome and predicted as predictor).

```{r, fig.height = 6}

summaries_calib_slope |> 
  plot_results_by_model(CI = TRUE) + ylab("Calibration Slope")

```

## Conclusions

1.  When there is level 2 endogeneity we see bias in beta estimates.

This bias does not reduce with an increasing number of studies but it does reduce with higher study size. This is consistent with the literature, which shows (Castelano 2014) (analytically shown for generalised least squared estimates in Maddala 1971). Intuitively the random intercept estimate of beta can be seen as pooling between cluster and within cluster info in beta. Bias arises from correlation between the between cluster variation in x and intercepts. This leads to bias in the between cluster estimates of beta. When cluster size is large, little weight is given to between cluster estimates and the unbiased within cluster estimates dominate.

2.  In these simulations, sometimes bias was greater when ICC = 0.05 compared to ICC = 0.3. This may be an artifact of a greater assosiation needed between predictors and intercepts to give the same correlation when intercepts were less variable.

3.  The impact of bias arising form level 2 endogeneity on mean squared error for betas, and on overall R-squared is small.

There is however an impact on calibration slope.

3.  Differences between the handling of between- and within- study info for fixed effects models and random intercept models are important

On reflection (and reading RHS section 3.8 p158), the data generating model chosen for my simulations favors fixed effects models due to predictors varying within clusters but not between clusters. Fixed effects models only use within cluster info, this is large in my data as this is where the variation in predictors occurs. If there is more between cluster variation in predictors (and within cluster and between cluster effects are identical/similar) then the random intercept model can have lower mean squared error as it can make use of between cluster info. This would occur if clusters were homogeneous but different, for example if we have studies recruited based on intellectual disability with little variation within clusters. I also assume constant within and between cluster effects. If there is a differing between cluster effect of a predictor (how could this occur - could be driven by selection eg. clinical samples have lower IQ, but also worse outcomes because of other unmeasured factors which are lower in clinical samples)

Recommendations as to which model to use should be driven by whether level 2 endogeneity is thought to be a problem (an additional assumption from random intercept models), but also whether there is between cluster information available. This will particularly be the case when there are lots of smaller clusters. In general for IPD meta-analysis there is more variation within studies than between studies and level 2 endogeneity is likely to be an issue as differing selection may have occurred \[find papers\].

The situation may be more complicated if random slopes for predictors are included, as these may remain biased due to level 2 endogeneity.

4.  Other thoughts

To many dimensions to think about. What about bias variance. Think about searching for daylight between FE and RE models. What are the impliacitons for individual level prediction of between-study information - does this fundamentally focus on within cluster effects? What if the variance of predictors varies with intercept.

```{r}
dbDisconnect(db)

```
---
title: "Plots for thesis chapter"
format: html
editor: visual
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:

```{r}
1 + 1
```

You can add options to executable code like this

```{r}
#| echo: false
2 * 2
```

The `echo: false` option disables the printing of code (only output is displayed).
